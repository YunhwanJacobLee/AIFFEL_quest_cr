{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2205719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버전: 2.5.1+cu121\n",
      "GPU 사용 가능: True\n",
      "GPU 이름: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"버전:\", torch.__version__)\n",
    "print(\"GPU 사용 가능:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a20f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Iter 351\n",
      "Epoch Time: 68 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 258\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    257\u001b[0m fixed_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 258\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar_discriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdis_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_seeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_noise\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[170], line 228\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, generator, discriminator, gen_optimizer, dis_optimizer, criterion, device, epochs, save_every, sample_seeds)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it, (real_images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m    227\u001b[0m     real_images \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 228\u001b[0m     gen_loss, dis_loss, real_acc, fake_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdis_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(gen_loss)\n\u001b[0;32m    233\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisc_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(dis_loss)\n",
      "Cell \u001b[1;32mIn[170], line 177\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(real_images, generator, discriminator, gen_optimizer, dis_optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    174\u001b[0m gen_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    175\u001b[0m gen_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 177\u001b[0m real_acc, fake_acc \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_loss\u001b[38;5;241m.\u001b[39mitem(), dis_loss\u001b[38;5;241m.\u001b[39mitem(), real_acc, fake_acc\n",
      "Cell \u001b[1;32mIn[170], line 138\u001b[0m, in \u001b[0;36mdiscriminator_accuracy\u001b[1;34m(real_output, fake_output)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdiscriminator_accuracy\u001b[39m(real_output, fake_output):\n\u001b[1;32m--> 138\u001b[0m     real_acc \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mreal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     fake_acc \u001b[38;5;241m=\u001b[39m (fake_output \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m real_acc, fake_acc\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from IPython import display\n",
    "import imageio\n",
    "import glob\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# CIFAR-10 preprocessing\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Train 개수:\", len(train_dataset))\n",
    "print(\"Test 개수:\", len(test_dataset))\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 512, 4, 1, 0, bias=False),   # z: (B, 100, 1, 1)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),     # (B, 256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),     # (B, 128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),      # (B, 64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),        # (B, 3, 64, 64)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),      # 64x64 → 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),    # 32x32 → 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),   # 16x16 → 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),   # 8x8 → 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cifar_generator = Generator().to(device)\n",
    "cifar_generator.apply(weights_init)\n",
    "cifar_discriminator = Discriminator().to(device)\n",
    "cifar_discriminator.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "gen_optimizer = optim.Adam(cifar_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "dis_optimizer = optim.Adam(cifar_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Discriminator가 너무 강해지는 걸 막기 위해 label smoothing 추가\n",
    "def generator_loss(fake_output, criterion):\n",
    "    return criterion(fake_output, torch.ones_like(fake_output))  # 약한 정답 사용\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, criterion):\n",
    "    real_labels = torch.full_like(real_output, 0.9)  # soft label\n",
    "    fake_labels = torch.zeros_like(fake_output)\n",
    "    real_loss = criterion(real_output, real_labels)\n",
    "    fake_loss = criterion(fake_output, fake_labels)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def discriminator_accuracy(real_output, fake_output):\n",
    "    real_acc = (real_output >= 0.5).float().mean().item()\n",
    "    fake_acc = (fake_output < 0.5).float().mean().item()\n",
    "    return real_acc, fake_acc\n",
    "\n",
    "noise_dim = 100\n",
    "fixed_noise = torch.randn(64, noise_dim, device=device)\n",
    "\n",
    "def train_step(real_images, generator, discriminator, gen_optimizer, dis_optimizer, criterion, device):\n",
    "    batch_size = real_images.size(0)\n",
    "    noise = torch.randn(batch_size, noise_dim, 1, 1, device=device)\n",
    "\n",
    "    # Discriminator update\n",
    "    generator.eval()\n",
    "    discriminator.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "    real_output = discriminator(real_images)\n",
    "    fake_output = discriminator(fake_images)\n",
    "    dis_loss = discriminator_loss(real_output, fake_output, criterion)\n",
    "\n",
    "    dis_optimizer.zero_grad()\n",
    "    dis_loss.backward()\n",
    "    dis_optimizer.step()\n",
    "\n",
    "    # Generator update\n",
    "    generator.train()\n",
    "    discriminator.eval()\n",
    "\n",
    "    noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "    fake_images = generator(noise)\n",
    "    fake_output = discriminator(fake_images)\n",
    "    gen_loss = generator_loss(fake_output, criterion)\n",
    "\n",
    "    gen_optimizer.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_optimizer.step()\n",
    "\n",
    "    real_acc, fake_acc = discriminator_accuracy(real_output, fake_output)\n",
    "    return gen_loss.item(), dis_loss.item(), real_acc, fake_acc\n",
    "\n",
    "def generate_and_save_images(model, epoch, it, sample_seeds, save_dir):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(sample_seeds.to(next(model.parameters()).device))\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        img = predictions[i]\n",
    "        img = (img + 1) / 2\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(f'{save_dir}/sample_epoch_{epoch:04d}_iter_{it:03d}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "def draw_train_history(history, epoch, save_dir):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(history['gen_loss'], label='gen_loss')\n",
    "    plt.plot(history['disc_loss'], label='disc_loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Batch iters')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history['real_accuracy'], label='real_accuracy')\n",
    "    plt.plot(history['fake_accuracy'], label='fake_accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Discriminator Accuracy')\n",
    "    plt.xlabel('Batch iters')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(f'{save_dir}/train_history_{epoch:04d}.png')\n",
    "    plt.close()\n",
    "\n",
    "def train(dataset, generator, discriminator, gen_optimizer, dis_optimizer,\n",
    "          criterion, device, epochs=50, save_every=5, sample_seeds=None):\n",
    "    history = {'gen_loss':[], 'disc_loss':[], 'real_accuracy':[], 'fake_accuracy':[]}\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        for it, (real_images, _) in enumerate(dataset):\n",
    "            real_images = real_images.to(device)\n",
    "            gen_loss, dis_loss, real_acc, fake_acc = train_step(\n",
    "                real_images, generator, discriminator,\n",
    "                gen_optimizer, dis_optimizer, criterion, device\n",
    "            )\n",
    "            history['gen_loss'].append(gen_loss)\n",
    "            history['disc_loss'].append(dis_loss)\n",
    "            history['real_accuracy'].append(real_acc)\n",
    "            history['fake_accuracy'].append(fake_acc)\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                generate_and_save_images(generator, epoch+1, it+1, sample_seeds, './CIFAR-10/cifar_samples')\n",
    "                print(f\"Epoch {epoch+1} | Iter {it+1}\")\n",
    "                print(f\"Epoch Time: {int(time.time() - epoch_start)} sec\")\n",
    "\n",
    "        checkpoint_dir = './CIFAR-10/cifar_training_checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(generator.state_dict(), f'{checkpoint_dir}/generator_epoch_{epoch+1}.pth')\n",
    "            torch.save(discriminator.state_dict(), f'{checkpoint_dir}/discriminator_epoch_{epoch+1}.pth')\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epoch+1, it+1, sample_seeds, './CIFAR-10/cifar_samples')\n",
    "        draw_train_history(history, epoch+1, './CIFAR-10/cifar_training_history')\n",
    "        print(f\"Epoch {epoch+1} completed in {int(time.time() - epoch_start)} sec\")\n",
    "\n",
    "    print(f\"Training completed in {int(time.time() - start)} sec\")\n",
    "\n",
    "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "train(train_loader, cifar_generator, cifar_discriminator,\n",
    "      gen_optimizer, dis_optimizer, criterion,\n",
    "      device, epochs=50, save_every=5, sample_seeds=fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cbced6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train 개수: 50000\n",
      "Test 개수: 10000\n",
      "[0/25][0/391] Loss_D: 1.7231 Loss_G: 3.0261\n",
      "[0/25][100/391] Loss_D: 1.3020 Loss_G: 25.0515\n",
      "[0/25][200/391] Loss_D: 0.4529 Loss_G: 5.6917\n",
      "[0/25][300/391] Loss_D: 0.0420 Loss_G: 6.1406\n",
      "[1/25][0/391] Loss_D: 0.6738 Loss_G: 5.0536\n",
      "[1/25][100/391] Loss_D: 1.9142 Loss_G: 7.6141\n",
      "[1/25][200/391] Loss_D: 0.1519 Loss_G: 4.1469\n",
      "[1/25][300/391] Loss_D: 0.6503 Loss_G: 6.2011\n",
      "[2/25][0/391] Loss_D: 0.5857 Loss_G: 3.0665\n",
      "[2/25][100/391] Loss_D: 0.4523 Loss_G: 3.5095\n",
      "[2/25][200/391] Loss_D: 0.8105 Loss_G: 2.7295\n",
      "[2/25][300/391] Loss_D: 0.4686 Loss_G: 3.7898\n",
      "[3/25][0/391] Loss_D: 0.2508 Loss_G: 3.6535\n",
      "[3/25][100/391] Loss_D: 1.3923 Loss_G: 8.5584\n",
      "[3/25][200/391] Loss_D: 0.6312 Loss_G: 4.9120\n",
      "[3/25][300/391] Loss_D: 0.3110 Loss_G: 4.0873\n",
      "[4/25][0/391] Loss_D: 0.5139 Loss_G: 3.7690\n",
      "[4/25][100/391] Loss_D: 0.4185 Loss_G: 3.4644\n",
      "[4/25][200/391] Loss_D: 0.3504 Loss_G: 3.1024\n",
      "[4/25][300/391] Loss_D: 0.4199 Loss_G: 2.7921\n",
      "[5/25][0/391] Loss_D: 0.4619 Loss_G: 3.0893\n",
      "[5/25][100/391] Loss_D: 1.4572 Loss_G: 0.6395\n",
      "[5/25][200/391] Loss_D: 1.1138 Loss_G: 3.8221\n",
      "[5/25][300/391] Loss_D: 1.1958 Loss_G: 1.2503\n",
      "[6/25][0/391] Loss_D: 0.7219 Loss_G: 2.5976\n",
      "[6/25][100/391] Loss_D: 0.6932 Loss_G: 3.6154\n",
      "[6/25][200/391] Loss_D: 0.7025 Loss_G: 5.0132\n",
      "[6/25][300/391] Loss_D: 1.0954 Loss_G: 4.9537\n",
      "[7/25][0/391] Loss_D: 0.5643 Loss_G: 3.2728\n",
      "[7/25][100/391] Loss_D: 0.3505 Loss_G: 3.5228\n",
      "[7/25][200/391] Loss_D: 1.7350 Loss_G: 6.9874\n",
      "[7/25][300/391] Loss_D: 0.4476 Loss_G: 2.6903\n",
      "[8/25][0/391] Loss_D: 1.1082 Loss_G: 0.9184\n",
      "[8/25][100/391] Loss_D: 0.7195 Loss_G: 3.7915\n",
      "[8/25][200/391] Loss_D: 0.5479 Loss_G: 2.7636\n",
      "[8/25][300/391] Loss_D: 1.3449 Loss_G: 4.4381\n",
      "[9/25][0/391] Loss_D: 2.3035 Loss_G: 7.0632\n",
      "[9/25][100/391] Loss_D: 0.8152 Loss_G: 1.8440\n",
      "[9/25][200/391] Loss_D: 0.4752 Loss_G: 1.7151\n",
      "[9/25][300/391] Loss_D: 0.3575 Loss_G: 3.0130\n",
      "[10/25][0/391] Loss_D: 1.5818 Loss_G: 3.5836\n",
      "[10/25][100/391] Loss_D: 0.8246 Loss_G: 1.7237\n",
      "[10/25][200/391] Loss_D: 2.1804 Loss_G: 0.6615\n",
      "[10/25][300/391] Loss_D: 0.1914 Loss_G: 4.2947\n",
      "[11/25][0/391] Loss_D: 0.1311 Loss_G: 3.6574\n",
      "[11/25][100/391] Loss_D: 0.6179 Loss_G: 6.7733\n",
      "[11/25][200/391] Loss_D: 0.8449 Loss_G: 5.6935\n",
      "[11/25][300/391] Loss_D: 0.2504 Loss_G: 3.1745\n",
      "[12/25][0/391] Loss_D: 1.3228 Loss_G: 3.5995\n",
      "[12/25][100/391] Loss_D: 1.8570 Loss_G: 6.2169\n",
      "[12/25][200/391] Loss_D: 4.7930 Loss_G: 8.1714\n",
      "[12/25][300/391] Loss_D: 0.7565 Loss_G: 1.4461\n",
      "[13/25][0/391] Loss_D: 1.5422 Loss_G: 1.8746\n",
      "[13/25][100/391] Loss_D: 2.1341 Loss_G: 7.5707\n",
      "[13/25][200/391] Loss_D: 0.0623 Loss_G: 4.6242\n",
      "[13/25][300/391] Loss_D: 0.0683 Loss_G: 4.1955\n",
      "[14/25][0/391] Loss_D: 0.9662 Loss_G: 4.0559\n",
      "[14/25][100/391] Loss_D: 0.4488 Loss_G: 1.7523\n",
      "[14/25][200/391] Loss_D: 0.4617 Loss_G: 2.0350\n",
      "[14/25][300/391] Loss_D: 0.7241 Loss_G: 6.0358\n",
      "[15/25][0/391] Loss_D: 0.4375 Loss_G: 3.1272\n",
      "[15/25][100/391] Loss_D: 0.7053 Loss_G: 2.3674\n",
      "[15/25][200/391] Loss_D: 0.7100 Loss_G: 1.3503\n",
      "[15/25][300/391] Loss_D: 0.8937 Loss_G: 2.2433\n",
      "[16/25][0/391] Loss_D: 0.7248 Loss_G: 3.2576\n",
      "[16/25][100/391] Loss_D: 0.5743 Loss_G: 2.2065\n",
      "[16/25][200/391] Loss_D: 0.2598 Loss_G: 3.7485\n",
      "[16/25][300/391] Loss_D: 1.0372 Loss_G: 2.5309\n",
      "[17/25][0/391] Loss_D: 0.5980 Loss_G: 2.7943\n",
      "[17/25][100/391] Loss_D: 0.2718 Loss_G: 3.5477\n",
      "[17/25][200/391] Loss_D: 0.0968 Loss_G: 4.1975\n",
      "[17/25][300/391] Loss_D: 1.0803 Loss_G: 1.0620\n",
      "[18/25][0/391] Loss_D: 0.0614 Loss_G: 4.2980\n",
      "[18/25][100/391] Loss_D: 0.9827 Loss_G: 2.9571\n",
      "[18/25][200/391] Loss_D: 1.1893 Loss_G: 2.9263\n",
      "[18/25][300/391] Loss_D: 0.1287 Loss_G: 3.9131\n",
      "[19/25][0/391] Loss_D: 0.1621 Loss_G: 4.2573\n",
      "[19/25][100/391] Loss_D: 0.0654 Loss_G: 5.5092\n",
      "[19/25][200/391] Loss_D: 1.9935 Loss_G: 0.8215\n",
      "[19/25][300/391] Loss_D: 0.2035 Loss_G: 3.8064\n",
      "[20/25][0/391] Loss_D: 0.0190 Loss_G: 5.5366\n",
      "[20/25][100/391] Loss_D: 0.0360 Loss_G: 4.8162\n",
      "[20/25][200/391] Loss_D: 0.4386 Loss_G: 2.9224\n",
      "[20/25][300/391] Loss_D: 0.3819 Loss_G: 3.6267\n",
      "[21/25][0/391] Loss_D: 0.9577 Loss_G: 1.2859\n",
      "[21/25][100/391] Loss_D: 1.2229 Loss_G: 1.5155\n",
      "[21/25][200/391] Loss_D: 0.1281 Loss_G: 3.5835\n",
      "[21/25][300/391] Loss_D: 0.5264 Loss_G: 2.1637\n",
      "[22/25][0/391] Loss_D: 0.1485 Loss_G: 4.2693\n",
      "[22/25][100/391] Loss_D: 0.7084 Loss_G: 1.6428\n",
      "[22/25][200/391] Loss_D: 0.0830 Loss_G: 4.6444\n",
      "[22/25][300/391] Loss_D: 0.5874 Loss_G: 3.5722\n",
      "[23/25][0/391] Loss_D: 0.0232 Loss_G: 4.9783\n",
      "[23/25][100/391] Loss_D: 0.5815 Loss_G: 2.0759\n",
      "[23/25][200/391] Loss_D: 0.7054 Loss_G: 1.5269\n",
      "[23/25][300/391] Loss_D: 0.2854 Loss_G: 3.3511\n",
      "[24/25][0/391] Loss_D: 0.0610 Loss_G: 4.6851\n",
      "[24/25][100/391] Loss_D: 0.8416 Loss_G: 3.1966\n",
      "[24/25][200/391] Loss_D: 0.8098 Loss_G: 2.0068\n",
      "[24/25][300/391] Loss_D: 0.0123 Loss_G: 6.5016\n",
      "✅ 학습 완료\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from IPython import display\n",
    "import imageio\n",
    "import glob\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# CIFAR-10 preprocessing\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"Train 개수:\", len(train_dataset))\n",
    "print(\"Test 개수:\", len(test_dataset))\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 512, 4, 1, 0, bias=False),   # z: (B, 100, 1, 1)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),     # (B, 256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),     # (B, 128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),      # (B, 64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),        # (B, 3, 64, 64)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),      # 64x64 → 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),    # 32x32 → 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),   # 16x16 → 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),   # 8x8 → 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cifar_generator = Generator().to(device)\n",
    "cifar_generator.apply(weights_init)\n",
    "cifar_discriminator = Discriminator().to(device)\n",
    "cifar_discriminator.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(cifar_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "dis_optimizer = optim.Adam(cifar_discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# 고정 시드로 생성할 노이즈\n",
    "fixed_noise = torch.randn(64, 100, 1, 1, device=device)\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # 진짜/가짜 라벨\n",
    "        real_labels = torch.full((batch_size,), 1.0, device=device)\n",
    "        fake_labels = torch.full((batch_size,), 0.0, device=device)\n",
    "\n",
    "        #### 1. Discriminator 학습 ####\n",
    "        cifar_discriminator.zero_grad()\n",
    "\n",
    "        # 진짜 이미지\n",
    "        output_real = cifar_discriminator(real_images).view(-1)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "\n",
    "        # 가짜 이미지\n",
    "        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "        fake_images = cifar_generator(noise)\n",
    "        output_fake = cifar_discriminator(fake_images.detach()).view(-1)\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "        # 총 loss 계산 및 역전파\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        #### 2. Generator 학습 ####\n",
    "        cifar_generator.zero_grad()\n",
    "        output = cifar_discriminator(fake_images).view(-1)\n",
    "        loss_G = criterion(output, real_labels)  # Generator는 진짜처럼 보이길 원함\n",
    "        loss_G.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[{epoch}/{epochs}][{i}/{len(train_loader)}] Loss_D: {loss_D.item():.4f} Loss_G: {loss_G.item():.4f}\")\n",
    "\n",
    "    # 에폭 끝날 때 이미지 저장\n",
    "    with torch.no_grad():\n",
    "        fake = cifar_generator(fixed_noise).detach().cpu()\n",
    "    os.makedirs(\"cifar_samples\", exist_ok=True)\n",
    "    torchvision.utils.save_image(fake, f\"cifar_samples/fake_epoch_{epoch:03d}.png\", normalize=True)\n",
    "\n",
    "print(\"✅ 학습 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db71bb4",
   "metadata": {},
   "source": [
    "### 회고\n",
    "- 32 * 32 이미지, batch = 256, 32 * 32 이미지, batch = 128, 64 * 64 이미지, batch = 128 등 다양한 시도를 하였으나 형상이 나타나지 않은 문제가 발생함.\n",
    "- 구글에 있는 코드(test.ipynb)를 사용해서 돌려보니 비슷하지는 않지만, 이미지 생성이 되는 것을 확인함.\n",
    "- lms 기반 코드와 구글에 있는 코드를 비교하면서 최대한 맞추려고 했으나 문제를 해결하지 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bfcaf5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
