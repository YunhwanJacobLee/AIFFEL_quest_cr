{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80733145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  653\n",
      "Files already downloaded and verified\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "[0/25][0/391] Loss_D: 1.3225 Loss_G: 6.9616 D(x): 0.6880 D(G(z)): 0.5293 / 0.0015\n",
      "saving the output\n",
      "[0/25][1/391] Loss_D: 1.0946 Loss_G: 4.6718 D(x): 0.5711 D(G(z)): 0.3066 / 0.0134\n",
      "[0/25][2/391] Loss_D: 1.5884 Loss_G: 5.8472 D(x): 0.7582 D(G(z)): 0.6372 / 0.0048\n",
      "[0/25][3/391] Loss_D: 0.9396 Loss_G: 6.1565 D(x): 0.6731 D(G(z)): 0.2946 / 0.0033\n",
      "[0/25][4/391] Loss_D: 0.9027 Loss_G: 6.3074 D(x): 0.7448 D(G(z)): 0.2839 / 0.0031\n",
      "[0/25][5/391] Loss_D: 0.9360 Loss_G: 7.6073 D(x): 0.7655 D(G(z)): 0.3785 / 0.0008\n",
      "[0/25][6/391] Loss_D: 0.7776 Loss_G: 7.0995 D(x): 0.7274 D(G(z)): 0.2164 / 0.0014\n",
      "[0/25][7/391] Loss_D: 1.1173 Loss_G: 9.2260 D(x): 0.7760 D(G(z)): 0.4725 / 0.0002\n",
      "[0/25][8/391] Loss_D: 0.8171 Loss_G: 7.1322 D(x): 0.6486 D(G(z)): 0.1223 / 0.0014\n",
      "[0/25][9/391] Loss_D: 1.3490 Loss_G: 10.8308 D(x): 0.8394 D(G(z)): 0.6101 / 0.0000\n",
      "[0/25][10/391] Loss_D: 0.4553 Loss_G: 8.3795 D(x): 0.7544 D(G(z)): 0.0454 / 0.0004\n",
      "[0/25][11/391] Loss_D: 0.8862 Loss_G: 10.1250 D(x): 0.8152 D(G(z)): 0.4025 / 0.0001\n",
      "[0/25][12/391] Loss_D: 0.2952 Loss_G: 9.1928 D(x): 0.8765 D(G(z)): 0.1083 / 0.0003\n",
      "[0/25][13/391] Loss_D: 0.4849 Loss_G: 9.8493 D(x): 0.8811 D(G(z)): 0.2446 / 0.0001\n",
      "[0/25][14/391] Loss_D: 0.4312 Loss_G: 9.0440 D(x): 0.8189 D(G(z)): 0.1379 / 0.0002\n",
      "[0/25][15/391] Loss_D: 0.6380 Loss_G: 10.5164 D(x): 0.8020 D(G(z)): 0.2395 / 0.0001\n",
      "[0/25][16/391] Loss_D: 0.4637 Loss_G: 9.1253 D(x): 0.8163 D(G(z)): 0.0995 / 0.0002\n",
      "[0/25][17/391] Loss_D: 0.7680 Loss_G: 14.2402 D(x): 0.8641 D(G(z)): 0.4001 / 0.0000\n",
      "[0/25][18/391] Loss_D: 0.2481 Loss_G: 11.4393 D(x): 0.8216 D(G(z)): 0.0048 / 0.0000\n",
      "[0/25][19/391] Loss_D: 0.3565 Loss_G: 9.2488 D(x): 0.8643 D(G(z)): 0.1504 / 0.0002\n",
      "[0/25][20/391] Loss_D: 0.9775 Loss_G: 17.7632 D(x): 0.9092 D(G(z)): 0.5313 / 0.0000\n",
      "[0/25][21/391] Loss_D: 0.2274 Loss_G: 17.1194 D(x): 0.8297 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][22/391] Loss_D: 0.2756 Loss_G: 10.2995 D(x): 0.8032 D(G(z)): 0.0004 / 0.0001\n",
      "[0/25][23/391] Loss_D: 0.4832 Loss_G: 12.5661 D(x): 0.9452 D(G(z)): 0.3093 / 0.0000\n",
      "[0/25][24/391] Loss_D: 0.1591 Loss_G: 10.2983 D(x): 0.8874 D(G(z)): 0.0160 / 0.0001\n",
      "[0/25][25/391] Loss_D: 0.5145 Loss_G: 14.4905 D(x): 0.8856 D(G(z)): 0.2811 / 0.0000\n",
      "[0/25][26/391] Loss_D: 0.1613 Loss_G: 11.6910 D(x): 0.8762 D(G(z)): 0.0043 / 0.0000\n",
      "[0/25][27/391] Loss_D: 0.2589 Loss_G: 12.6261 D(x): 0.9568 D(G(z)): 0.1759 / 0.0000\n",
      "[0/25][28/391] Loss_D: 0.1038 Loss_G: 9.9566 D(x): 0.9361 D(G(z)): 0.0272 / 0.0001\n",
      "[0/25][29/391] Loss_D: 0.4163 Loss_G: 17.4228 D(x): 0.9544 D(G(z)): 0.2840 / 0.0000\n",
      "[0/25][30/391] Loss_D: 0.1767 Loss_G: 16.2667 D(x): 0.8589 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][31/391] Loss_D: 0.1252 Loss_G: 9.1947 D(x): 0.8990 D(G(z)): 0.0016 / 0.0002\n",
      "[0/25][32/391] Loss_D: 1.8555 Loss_G: 24.8913 D(x): 0.9367 D(G(z)): 0.7869 / 0.0000\n",
      "[0/25][33/391] Loss_D: 0.2868 Loss_G: 27.5226 D(x): 0.7974 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][34/391] Loss_D: 0.4144 Loss_G: 24.7191 D(x): 0.7274 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][35/391] Loss_D: 0.0559 Loss_G: 17.0301 D(x): 0.9490 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][36/391] Loss_D: 0.0184 Loss_G: 7.5423 D(x): 0.9832 D(G(z)): 0.0012 / 0.0010\n",
      "[0/25][37/391] Loss_D: 2.0098 Loss_G: 24.0281 D(x): 0.9836 D(G(z)): 0.8245 / 0.0000\n",
      "[0/25][38/391] Loss_D: 0.0927 Loss_G: 27.7496 D(x): 0.9363 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][39/391] Loss_D: 0.1678 Loss_G: 26.7714 D(x): 0.8723 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][40/391] Loss_D: 0.2636 Loss_G: 22.5402 D(x): 0.8286 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][41/391] Loss_D: 0.0636 Loss_G: 14.6360 D(x): 0.9506 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][42/391] Loss_D: 0.0482 Loss_G: 5.6081 D(x): 0.9640 D(G(z)): 0.0086 / 0.0069\n",
      "[0/25][43/391] Loss_D: 3.6441 Loss_G: 24.8804 D(x): 0.9633 D(G(z)): 0.9606 / 0.0000\n",
      "[0/25][44/391] Loss_D: 0.1580 Loss_G: 29.4281 D(x): 0.8865 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][45/391] Loss_D: 0.2738 Loss_G: 29.8076 D(x): 0.7903 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][46/391] Loss_D: 0.1206 Loss_G: 28.5659 D(x): 0.8991 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][47/391] Loss_D: 0.0915 Loss_G: 25.8043 D(x): 0.9295 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][48/391] Loss_D: 0.0294 Loss_G: 20.5645 D(x): 0.9716 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][49/391] Loss_D: 0.0316 Loss_G: 13.9270 D(x): 0.9755 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][50/391] Loss_D: 0.0280 Loss_G: 5.8549 D(x): 0.9824 D(G(z)): 0.0090 / 0.0071\n",
      "[0/25][51/391] Loss_D: 1.7890 Loss_G: 21.8666 D(x): 0.9857 D(G(z)): 0.7822 / 0.0000\n",
      "[0/25][52/391] Loss_D: 0.1600 Loss_G: 25.7743 D(x): 0.8845 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][53/391] Loss_D: 0.5042 Loss_G: 24.2660 D(x): 0.7753 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][54/391] Loss_D: 0.1526 Loss_G: 19.8137 D(x): 0.9015 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][55/391] Loss_D: 0.0878 Loss_G: 12.6396 D(x): 0.9268 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][56/391] Loss_D: 0.0530 Loss_G: 5.3460 D(x): 0.9648 D(G(z)): 0.0118 / 0.0108\n",
      "[0/25][57/391] Loss_D: 1.5602 Loss_G: 18.0817 D(x): 0.9633 D(G(z)): 0.7138 / 0.0000\n",
      "[0/25][58/391] Loss_D: 0.3523 Loss_G: 20.8812 D(x): 0.8006 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][59/391] Loss_D: 0.5016 Loss_G: 17.9335 D(x): 0.7815 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][60/391] Loss_D: 0.1018 Loss_G: 12.7966 D(x): 0.9425 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][61/391] Loss_D: 0.0542 Loss_G: 6.9494 D(x): 0.9693 D(G(z)): 0.0032 / 0.0027\n",
      "[0/25][62/391] Loss_D: 0.6158 Loss_G: 10.5701 D(x): 0.9751 D(G(z)): 0.3905 / 0.0001\n",
      "[0/25][63/391] Loss_D: 0.1541 Loss_G: 9.6472 D(x): 0.8973 D(G(z)): 0.0079 / 0.0001\n",
      "[0/25][64/391] Loss_D: 0.1073 Loss_G: 6.7797 D(x): 0.9313 D(G(z)): 0.0204 / 0.0017\n",
      "[0/25][65/391] Loss_D: 1.2288 Loss_G: 17.7958 D(x): 0.9423 D(G(z)): 0.5966 / 0.0000\n",
      "[0/25][66/391] Loss_D: 1.0753 Loss_G: 15.9597 D(x): 0.5372 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][67/391] Loss_D: 0.2999 Loss_G: 9.4319 D(x): 0.8162 D(G(z)): 0.0012 / 0.0007\n",
      "[0/25][68/391] Loss_D: 0.4557 Loss_G: 10.5775 D(x): 0.9785 D(G(z)): 0.3049 / 0.0000\n",
      "[0/25][69/391] Loss_D: 0.2144 Loss_G: 8.5218 D(x): 0.8554 D(G(z)): 0.0321 / 0.0003\n",
      "[0/25][70/391] Loss_D: 0.2000 Loss_G: 8.9997 D(x): 0.9381 D(G(z)): 0.1063 / 0.0003\n",
      "[0/25][71/391] Loss_D: 0.2453 Loss_G: 6.4587 D(x): 0.8680 D(G(z)): 0.0523 / 0.0036\n",
      "[0/25][72/391] Loss_D: 0.4185 Loss_G: 11.4062 D(x): 0.9315 D(G(z)): 0.2542 / 0.0000\n",
      "[0/25][73/391] Loss_D: 0.5585 Loss_G: 7.3151 D(x): 0.7400 D(G(z)): 0.0020 / 0.0013\n",
      "[0/25][74/391] Loss_D: 0.2857 Loss_G: 6.2280 D(x): 0.9100 D(G(z)): 0.1208 / 0.0039\n",
      "[0/25][75/391] Loss_D: 0.3730 Loss_G: 10.1785 D(x): 0.9622 D(G(z)): 0.2526 / 0.0001\n",
      "[0/25][76/391] Loss_D: 0.1858 Loss_G: 8.1097 D(x): 0.8699 D(G(z)): 0.0094 / 0.0007\n",
      "[0/25][77/391] Loss_D: 0.2556 Loss_G: 5.2935 D(x): 0.8961 D(G(z)): 0.0827 / 0.0091\n",
      "[0/25][78/391] Loss_D: 0.5413 Loss_G: 7.9592 D(x): 0.8842 D(G(z)): 0.2687 / 0.0006\n",
      "[0/25][79/391] Loss_D: 0.3026 Loss_G: 5.9690 D(x): 0.8113 D(G(z)): 0.0266 / 0.0039\n",
      "[0/25][80/391] Loss_D: 0.3975 Loss_G: 7.1090 D(x): 0.9051 D(G(z)): 0.2289 / 0.0014\n",
      "[0/25][81/391] Loss_D: 0.3258 Loss_G: 5.3916 D(x): 0.8251 D(G(z)): 0.0442 / 0.0076\n",
      "[0/25][82/391] Loss_D: 0.4327 Loss_G: 9.5565 D(x): 0.9589 D(G(z)): 0.3006 / 0.0001\n",
      "[0/25][83/391] Loss_D: 0.6101 Loss_G: 4.9934 D(x): 0.6559 D(G(z)): 0.0083 / 0.0121\n",
      "[0/25][84/391] Loss_D: 0.4876 Loss_G: 8.8470 D(x): 0.9481 D(G(z)): 0.3276 / 0.0002\n",
      "[0/25][85/391] Loss_D: 0.4781 Loss_G: 5.7586 D(x): 0.7115 D(G(z)): 0.0137 / 0.0042\n",
      "[0/25][86/391] Loss_D: 0.2064 Loss_G: 5.4503 D(x): 0.9448 D(G(z)): 0.1264 / 0.0066\n",
      "[0/25][87/391] Loss_D: 0.3436 Loss_G: 8.0604 D(x): 0.9450 D(G(z)): 0.2295 / 0.0006\n",
      "[0/25][88/391] Loss_D: 0.5426 Loss_G: 3.7429 D(x): 0.7071 D(G(z)): 0.0398 / 0.0403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 155\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m###########################\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# train with real\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     netD\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 155\u001b[0m     real_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m real_cpu\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    157\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), real_label, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "#set manual seed to a constant get a consistent output\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "#loading the dataset\n",
    "dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(64),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "nc=3\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "#checking the availability of cuda devices\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# number of gpu's available\n",
    "ngpu = 1\n",
    "# input noise dimension\n",
    "nz = 100\n",
    "# number of generator filters\n",
    "ngf = 64\n",
    "#number of discriminator filters\n",
    "ndf = 64\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "            return output\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "#load weights to test the model\n",
    "#netG.load_state_dict(torch.load('weights/netG_epoch_24.pth'))\n",
    "print(netG)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "#load weights to test the model \n",
    "#netD.load_state_dict(torch.load('weights/netD_epoch_24.pth'))\n",
    "print(netD)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(128, nz, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "niter = 25\n",
    "g_loss = []\n",
    "d_loss = []\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device, dtype=torch.float)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        #save the output\n",
    "        if i % 100 == 0:\n",
    "            print('saving the output')\n",
    "            os.makedirs('output', exist_ok=True)\n",
    "            vutils.save_image(real_cpu,'output/real_samples.png',normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),'output/fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n",
    "    \n",
    "    # Check pointing for every epoch\n",
    "    os.makedirs('weights', exist_ok=True)\n",
    "    torch.save(netG.state_dict(), 'weights/netG_epoch_%d.pth' % (epoch))\n",
    "    torch.save(netD.state_dict(), 'weights/netD_epoch_%d.pth' % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DM\\AppData\\Local\\Temp\\ipykernel_19524\\2548883800.py:11: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images = [imageio.imread(path) for path in image_paths]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to output/dcgan_training.gif\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import glob\n",
    "\n",
    "# 저장된 이미지 경로 패턴 (epoch 순서대로 정렬되도록)\n",
    "image_paths = sorted(glob.glob('output/fake_samples_epoch_*.png'))\n",
    "\n",
    "# GIF 저장 경로\n",
    "gif_path = 'output/dcgan_training.gif'\n",
    "\n",
    "# 이미지들을 읽고 gif로 저장\n",
    "images = [imageio.imread(path) for path in image_paths]\n",
    "imageio.mimsave(gif_path, images, fps=5)\n",
    "\n",
    "print(f\"GIF saved to {gif_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d08fb4",
   "metadata": {},
   "source": [
    "### 회고\n",
    "- 32 * 32 이미지, batch = 256, 32 * 32 이미지, batch = 128, 64 * 64 이미지, batch = 128 등 다양한 시도를 하였으나 형상이 나타나지 않은 문제가 발생함.\n",
    "- 구글에 있는 코드를 사용해서 돌려보니 비슷하지는 않지만, 이미지 생성이 되는 것을 확인함.\n",
    "- lms 기반 코드와 구글에 있는 코드를 비교하면서 최대한 맞추려고 했으나 문제를 해결하지 못함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc30100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
