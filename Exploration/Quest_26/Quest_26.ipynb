{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25fc493",
   "metadata": {},
   "source": [
    "## 1. 기본 설정 및 라이브러리 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5f576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d388f",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78cf9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '~/aiffel/transformer_chatbot/data/ChatbotData .csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# 결측치 제거\n",
    "data = data.dropna()\n",
    "\n",
    "# 질문/답변 분리\n",
    "questions = data['Q'].tolist()\n",
    "answers = data['A'].tolist()\n",
    "\n",
    "############# 루브릭 1. ##############\n",
    "# 한국어 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "questions = [preprocess_sentence(q) for q in questions]\n",
    "answers = [preprocess_sentence(a) for a in answers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89a18b",
   "metadata": {},
   "source": [
    "## 3. SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "632d328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "def tokenize_and_filter(inputs, outputs, max_len=40):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for sentence1, sentence2 in zip(inputs, outputs):\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        if len(sentence1) <= max_len and len(sentence2) <= max_len:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=max_len, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=max_len, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be724cb",
   "metadata": {},
   "source": [
    "## 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35692bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 멀티 헤드 어텐션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    "\n",
    "# 포지셔널 인코딩\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        pos = np.arange(position)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rads = self.get_angles(pos, i, d_model)\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# 패딩 마스크\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# 룩어헤드 마스크\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "# 인코더 레이어\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn_output = self.mha({'query': x, 'key': x, 'value': x, 'mask': mask})\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "# 디코더 레이어\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        attn1 = self.mha1({'query': x, 'key': x, 'value': x, 'mask': look_ahead_mask})\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(x + attn1)\n",
    "\n",
    "        attn2 = self.mha2({'query': out1, 'key': enc_output, 'value': enc_output, 'mask': padding_mask})\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(out1 + attn2)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "\n",
    "        return out3\n",
    "\n",
    "# 인코더\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        return x\n",
    "\n",
    "# 디코더\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "        return x\n",
    "\n",
    "# 트랜스포머\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, pe_input, pe_target):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                               input_vocab_size, pe_input)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inputs['inputs'], inputs['dec_inputs'])\n",
    "\n",
    "        enc_output = self.encoder(inputs['inputs'], enc_padding_mask)\n",
    "        dec_output = self.decoder(inputs['dec_inputs'], enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output\n",
    "\n",
    "# 마스크 생성기\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3e8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restored: ./checkpoints/train/ckpt-60\n",
      "Epoch 1 Batch 0 Loss 0.0144 Accuracy 0.9955\n",
      "Epoch 1 Batch 100 Loss 0.0320 Accuracy 0.9953\n",
      "Epoch 1 Loss 0.0252 Accuracy 0.9950\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-61\n",
      "Epoch 2 Batch 0 Loss 0.0224 Accuracy 0.9954\n",
      "Epoch 2 Batch 100 Loss 0.0369 Accuracy 0.9953\n",
      "Epoch 2 Loss 0.0237 Accuracy 0.9952\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-62\n",
      "Epoch 3 Batch 0 Loss 0.0103 Accuracy 0.9976\n",
      "Epoch 3 Batch 100 Loss 0.0096 Accuracy 0.9976\n",
      "Epoch 3 Loss 0.0244 Accuracy 0.9952\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-63\n",
      "Epoch 4 Batch 0 Loss 0.0406 Accuracy 0.9911\n",
      "Epoch 4 Batch 100 Loss 0.0107 Accuracy 0.9977\n",
      "Epoch 4 Loss 0.0214 Accuracy 0.9958\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-64\n",
      "Epoch 5 Batch 0 Loss 0.0349 Accuracy 0.9887\n",
      "Epoch 5 Batch 100 Loss 0.0159 Accuracy 0.9908\n",
      "Epoch 5 Loss 0.0218 Accuracy 0.9956\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-65\n",
      "Epoch 6 Batch 0 Loss 0.0163 Accuracy 0.9929\n",
      "Epoch 6 Batch 100 Loss 0.0297 Accuracy 0.9938\n",
      "Epoch 6 Loss 0.0236 Accuracy 0.9951\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-66\n",
      "Epoch 7 Batch 0 Loss 0.0351 Accuracy 0.9885\n",
      "Epoch 7 Batch 100 Loss 0.0225 Accuracy 0.9956\n",
      "Epoch 7 Loss 0.0203 Accuracy 0.9957\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/train/ckpt-67\n",
      "Epoch 8 Batch 0 Loss 0.0210 Accuracy 0.9952\n",
      "Epoch 8 Batch 100 Loss 0.0232 Accuracy 0.9950\n",
      "Epoch 8 Loss 0.0217 Accuracy 0.9957\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-68\n",
      "Epoch 9 Batch 0 Loss 0.0362 Accuracy 0.9908\n",
      "Epoch 9 Batch 100 Loss 0.0288 Accuracy 0.9922\n",
      "Epoch 9 Loss 0.0200 Accuracy 0.9958\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-69\n",
      "Epoch 10 Batch 0 Loss 0.0260 Accuracy 0.9928\n",
      "Epoch 10 Batch 100 Loss 0.0406 Accuracy 0.9957\n",
      "Epoch 10 Loss 0.0204 Accuracy 0.9959\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-70\n",
      "Epoch 11 Batch 0 Loss 0.0059 Accuracy 0.9976\n",
      "Epoch 11 Batch 100 Loss 0.0351 Accuracy 0.9889\n",
      "Epoch 11 Loss 0.0191 Accuracy 0.9959\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/train/ckpt-71\n",
      "Epoch 12 Batch 0 Loss 0.0342 Accuracy 0.9976\n",
      "Epoch 12 Batch 100 Loss 0.0064 Accuracy 1.0000\n",
      "Epoch 12 Loss 0.0195 Accuracy 0.9959\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/train/ckpt-72\n",
      "Epoch 13 Batch 0 Loss 0.0308 Accuracy 0.9930\n",
      "Epoch 13 Batch 100 Loss 0.0151 Accuracy 0.9977\n",
      "Epoch 13 Loss 0.0183 Accuracy 0.9960\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/train/ckpt-73\n",
      "Epoch 14 Batch 0 Loss 0.0059 Accuracy 1.0000\n",
      "Epoch 14 Batch 100 Loss 0.0180 Accuracy 0.9933\n",
      "Epoch 14 Loss 0.0181 Accuracy 0.9959\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/train/ckpt-74\n",
      "Epoch 15 Batch 0 Loss 0.0153 Accuracy 0.9976\n",
      "Epoch 15 Batch 100 Loss 0.0089 Accuracy 0.9954\n",
      "Epoch 15 Loss 0.0180 Accuracy 0.9960\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-75\n",
      "Epoch 16 Batch 0 Loss 0.0031 Accuracy 1.0000\n",
      "Epoch 16 Batch 100 Loss 0.0224 Accuracy 0.9977\n",
      "Epoch 16 Loss 0.0166 Accuracy 0.9964\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/train/ckpt-76\n",
      "Epoch 17 Batch 0 Loss 0.0067 Accuracy 0.9976\n",
      "Epoch 17 Batch 100 Loss 0.0057 Accuracy 1.0000\n",
      "Epoch 17 Loss 0.0169 Accuracy 0.9964\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/train/ckpt-77\n",
      "Epoch 18 Batch 0 Loss 0.0019 Accuracy 1.0000\n",
      "Epoch 18 Batch 100 Loss 0.0274 Accuracy 0.9910\n",
      "Epoch 18 Loss 0.0159 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/train/ckpt-78\n",
      "Epoch 19 Batch 0 Loss 0.0516 Accuracy 0.9932\n",
      "Epoch 19 Batch 100 Loss 0.0039 Accuracy 1.0000\n",
      "Epoch 19 Loss 0.0159 Accuracy 0.9967\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/train/ckpt-79\n",
      "Epoch 20 Batch 0 Loss 0.0139 Accuracy 0.9952\n",
      "Epoch 20 Batch 100 Loss 0.0031 Accuracy 1.0000\n",
      "Epoch 20 Loss 0.0176 Accuracy 0.9962\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-80\n"
     ]
    }
   ],
   "source": [
    "# 학습률 스케줄러\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    \n",
    "# 손실 함수\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # PAD는 무시\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # 패딩 마스크 반영\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "# 정확도 함수\n",
    "def accuracy_function(real, pred):\n",
    "    pred_ids = tf.cast(tf.argmax(pred, axis=2), dtype=real.dtype)  # 타입 일치\n",
    "\n",
    "    accuracies = tf.equal(real, pred_ids)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "d_model = 256  # 모델 차원 설정값에 맞게 수정\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "\n",
    "# 모델 하이퍼파라미터 정의\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# 토크나이저 관련 (이미 정의되어 있어야 함)\n",
    "INPUT_VOCAB_SIZE = VOCAB_SIZE\n",
    "TARGET_VOCAB_SIZE = VOCAB_SIZE\n",
    "MAX_POS_ENCODING = 40  # 최대 문장 길이\n",
    "\n",
    "\n",
    "# 트랜스포머 모델 생성\n",
    "transformer = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    input_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    target_vocab_size=TARGET_VOCAB_SIZE,\n",
    "    pe_input=MAX_POS_ENCODING,\n",
    "    pe_target=MAX_POS_ENCODING\n",
    ")\n",
    "\n",
    "\n",
    "# 체크포인트 설정\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 이전 체크포인트가 있으면 복원\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Checkpoint restored:\", ckpt_manager.latest_checkpoint)\n",
    "\n",
    "\n",
    "# 학습 루프 정의\n",
    "EPOCHS = 20\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    inp = inputs['inputs']\n",
    "    tar_inp = inputs['dec_inputs']\n",
    "    tar_real = inputs['outputs']\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(inputs, training=True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    acc = accuracy_function(tar_real, predictions)\n",
    "    return loss, acc\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더 입력은 정답에서 마지막 토큰 제외\n",
    "# 디코더 출력은 정답에서 시작 토큰 제외\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    inp = features['inputs']\n",
    "    tar_inp = features['dec_inputs']\n",
    "    tar_real = labels['outputs']\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(features, training=True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    acc = accuracy_function(tar_real, predictions)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "############# 루브릭 2. ##############\n",
    "# 학습 실행\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for batch, (features, labels) in enumerate(dataset):\n",
    "        batch_loss, batch_accuracy = train_step(features, labels)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Batch {batch} Loss {batch_loss:.4f} Accuracy {batch_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss {total_loss / (batch + 1):.4f} Accuracy {total_accuracy / (batch + 1):.4f}\")\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f\"Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7d0e9",
   "metadata": {},
   "source": [
    "## 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c122e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가용 함수\n",
    "def evaluate(sentence, model):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "    encoder_input = tf.expand_dims(sentence, 0)  # (1, 문장길이)\n",
    "\n",
    "    decoder_input = tf.expand_dims([START_TOKEN[0]], 0)  # (1, 1)\n",
    "    output = decoder_input\n",
    "\n",
    "    for i in range(MAX_POS_ENCODING):\n",
    "        predictions = transformer(\n",
    "            inputs={\n",
    "                'inputs': encoder_input,\n",
    "                'dec_inputs': output\n",
    "            },\n",
    "            training=False\n",
    "        )\n",
    "        predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "        predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id[0][0], END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# 실제 응답 생성 함수\n",
    "def predict(sentence, model):\n",
    "    prediction = evaluate(sentence, model)\n",
    "    \n",
    "    # END_TOKEN이 나오기 전까지만 디코딩\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "\n",
    "    print(f'User Input: {sentence}')\n",
    "    print(f'Chatbot Response: {predicted_sentence}')\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10103de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 너는 누구야?\n",
      "Chatbot Response: 저는 마음을 이어주는 위로봇입니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 마음을 이어주는 위로봇입니다 .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# 루브릭 3. ##############\n",
    "predict(\"너는 누구야?\", transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc5148b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 배고파\n",
      "Chatbot Response: 얼른 맛난 음식 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'얼른 맛난 음식 드세요 .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"배고파\", transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b42a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 지금은 11시 30분이고 점심시간은 12시 50분부터인데 지금부터 점심을 미리 먹어도 될까?\n",
      "Chatbot Response: 로맨틱하네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'로맨틱하네요 .'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"지금은 11시 30분이고 점심시간은 12시 50분부터인데 지금부터 점심을 미리 먹어도 될까?\", transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c87f470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 오호 문장이 길어지니까 너 말을 잘 이해를 못하는 것 같네?\n",
      "Chatbot Response: 노력에 따라 가능성이 달라지겠죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'노력에 따라 가능성이 달라지겠죠 .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"오호 문장이 길어지니까 너 말을 잘 이해를 못하는 것 같네?\", transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a50d39b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 노력에 따라라고? 나보고 널 더 잘 만들어보라는거야? 어떻게 해야 네가 더 똑똑해지는데?\n",
      "Chatbot Response: 중요한 건 노력하는 과정이에요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'중요한 건 노력하는 과정이에요 .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"노력에 따라라고? 나보고 널 더 잘 만들어보라는거야? 어떻게 해야 네가 더 똑똑해지는데?\", transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ba431",
   "metadata": {},
   "source": [
    "## 6. 하이퍼파라미터 변경 실험\n",
    "\n",
    "맥락과 상관없는 답변을 뱉기도 함   \n",
    "-> 하이퍼파라미터 변경 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5846f22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restored: ./checkpoints/train/ckpt-80\n",
      "Epoch 1 Batch 0 Loss 0.0072 Accuracy 0.9978\n",
      "Epoch 1 Batch 100 Loss 0.0221 Accuracy 0.9958\n",
      "Epoch 1 Loss 0.0156 Accuracy 0.9964\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-81\n",
      "Epoch 2 Batch 0 Loss 0.0064 Accuracy 1.0000\n",
      "Epoch 2 Batch 100 Loss 0.0051 Accuracy 0.9976\n",
      "Epoch 2 Loss 0.0148 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-82\n",
      "Epoch 3 Batch 0 Loss 0.0063 Accuracy 0.9978\n",
      "Epoch 3 Batch 100 Loss 0.0152 Accuracy 0.9977\n",
      "Epoch 3 Loss 0.0143 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-83\n",
      "Epoch 4 Batch 0 Loss 0.0203 Accuracy 0.9935\n",
      "Epoch 4 Batch 100 Loss 0.0361 Accuracy 0.9936\n",
      "Epoch 4 Loss 0.0147 Accuracy 0.9964\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-84\n",
      "Epoch 5 Batch 0 Loss 0.0155 Accuracy 0.9931\n",
      "Epoch 5 Batch 100 Loss 0.0209 Accuracy 0.9957\n",
      "Epoch 5 Loss 0.0146 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-85\n",
      "Epoch 6 Batch 0 Loss 0.0092 Accuracy 0.9933\n",
      "Epoch 6 Batch 100 Loss 0.0033 Accuracy 1.0000\n",
      "Epoch 6 Loss 0.0144 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-86\n",
      "Epoch 7 Batch 0 Loss 0.0032 Accuracy 0.9979\n",
      "Epoch 7 Batch 100 Loss 0.0192 Accuracy 0.9957\n",
      "Epoch 7 Loss 0.0140 Accuracy 0.9967\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/train/ckpt-87\n",
      "Epoch 8 Batch 0 Loss 0.0017 Accuracy 1.0000\n",
      "Epoch 8 Batch 100 Loss 0.0060 Accuracy 1.0000\n",
      "Epoch 8 Loss 0.0147 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-88\n",
      "Epoch 9 Batch 0 Loss 0.0015 Accuracy 1.0000\n",
      "Epoch 9 Batch 100 Loss 0.0029 Accuracy 1.0000\n",
      "Epoch 9 Loss 0.0129 Accuracy 0.9969\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-89\n",
      "Epoch 10 Batch 0 Loss 0.0063 Accuracy 0.9978\n",
      "Epoch 10 Batch 100 Loss 0.0034 Accuracy 1.0000\n",
      "Epoch 10 Loss 0.0124 Accuracy 0.9969\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-90\n",
      "Epoch 11 Batch 0 Loss 0.0161 Accuracy 0.9955\n",
      "Epoch 11 Batch 100 Loss 0.0328 Accuracy 0.9926\n",
      "Epoch 11 Loss 0.0135 Accuracy 0.9965\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/train/ckpt-91\n",
      "Epoch 12 Batch 0 Loss 0.0151 Accuracy 0.9957\n",
      "Epoch 12 Batch 100 Loss 0.0290 Accuracy 0.9933\n",
      "Epoch 12 Loss 0.0130 Accuracy 0.9967\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/train/ckpt-92\n",
      "Epoch 13 Batch 0 Loss 0.0009 Accuracy 1.0000\n",
      "Epoch 13 Batch 100 Loss 0.0224 Accuracy 0.9976\n",
      "Epoch 13 Loss 0.0119 Accuracy 0.9968\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/train/ckpt-93\n",
      "Epoch 14 Batch 0 Loss 0.0014 Accuracy 1.0000\n",
      "Epoch 14 Batch 100 Loss 0.0020 Accuracy 1.0000\n",
      "Epoch 14 Loss 0.0123 Accuracy 0.9966\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/train/ckpt-94\n",
      "Epoch 15 Batch 0 Loss 0.0030 Accuracy 1.0000\n",
      "Epoch 15 Batch 100 Loss 0.0280 Accuracy 0.9952\n",
      "Epoch 15 Loss 0.0115 Accuracy 0.9970\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-95\n",
      "Epoch 16 Batch 0 Loss 0.0043 Accuracy 1.0000\n",
      "Epoch 16 Batch 100 Loss 0.0125 Accuracy 0.9955\n",
      "Epoch 16 Loss 0.0120 Accuracy 0.9969\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/train/ckpt-96\n",
      "Epoch 17 Batch 0 Loss 0.0099 Accuracy 0.9979\n",
      "Epoch 17 Batch 100 Loss 0.0133 Accuracy 0.9948\n",
      "Epoch 17 Loss 0.0113 Accuracy 0.9970\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/train/ckpt-97\n",
      "Epoch 18 Batch 0 Loss 0.0037 Accuracy 0.9977\n",
      "Epoch 18 Batch 100 Loss 0.0042 Accuracy 1.0000\n",
      "Epoch 18 Loss 0.0115 Accuracy 0.9969\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/train/ckpt-98\n",
      "Epoch 19 Batch 0 Loss 0.0095 Accuracy 0.9956\n",
      "Epoch 19 Batch 100 Loss 0.0199 Accuracy 0.9935\n",
      "Epoch 19 Loss 0.0104 Accuracy 0.9970\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/train/ckpt-99\n",
      "Epoch 20 Batch 0 Loss 0.0150 Accuracy 0.9954\n",
      "Epoch 20 Batch 100 Loss 0.0246 Accuracy 0.9932\n",
      "Epoch 20 Loss 0.0104 Accuracy 0.9972\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-100\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 변경\n",
    "NUM_LAYERS = 4 ## 2 -> 4 상향 (더 깊은 네트워크로 문맥 관계 더 잘 학습)\n",
    "MAX_POS_ENCODING = 80 ## 40 -> 80 상향 (더 긴 문장 입출력 가능)\n",
    "\n",
    "# 트랜스포머 모델 생성\n",
    "transformer_2 = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    input_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    target_vocab_size=TARGET_VOCAB_SIZE,\n",
    "    pe_input=MAX_POS_ENCODING,\n",
    "    pe_target=MAX_POS_ENCODING\n",
    ")\n",
    "\n",
    "\n",
    "# 체크포인트 설정\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt_2 = tf.train.Checkpoint(transformer=transformer_2, optimizer=optimizer)\n",
    "ckpt_manager_2 = tf.train.CheckpointManager(ckpt_2, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 이전 체크포인트가 있으면 복원\n",
    "if ckpt_manager_2.latest_checkpoint:\n",
    "    ckpt_2.restore(ckpt_manager_2.latest_checkpoint)\n",
    "    print(\"Checkpoint restored:\", ckpt_manager_2.latest_checkpoint)\n",
    "    \n",
    "\n",
    "# 학습 실행\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for batch, (features, labels) in enumerate(dataset):\n",
    "        batch_loss, batch_accuracy = train_step(features, labels)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Batch {batch} Loss {batch_loss:.4f} Accuracy {batch_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss {total_loss / (batch + 1):.4f} Accuracy {total_accuracy / (batch + 1):.4f}\")\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    ckpt_save_path = ckpt_manager_2.save()\n",
    "    print(f\"Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "542be87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 너는 누구야?\n",
      "Chatbot Response: 저는 마음을 이어주는 위로봇입니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 마음을 이어주는 위로봇입니다 .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"너는 누구야?\", transformer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09e5766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 배고파\n",
      "Chatbot Response: 뭐 좀 챙겨드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'뭐 좀 챙겨드세요 .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"배고파\", transformer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59a848b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 지금은 11시 30분이고 점심시간은 12시 50분부터인데 지금부터 점심을 미리 먹어도 될까?\n",
      "Chatbot Response: 은 마음을 는 여러가지 이유가 있기 마련이죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'은 마음을 는 여러가지 이유가 있기 마련이죠 .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"지금은 11시 30분이고 점심시간은 12시 50분부터인데 지금부터 점심을 미리 먹어도 될까?\", transformer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcd9b24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 오호 문장이 길어지니까 너 말을 잘 이해를 못하는 것 같네?\n",
      "Chatbot Response: 솔직함으로 사랑을 쟁취하세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'솔직함으로 사랑을 쟁취하세요 .'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"오호 문장이 길어지니까 너 말을 잘 이해를 못하는 것 같네?\", transformer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "942df964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 노력에 따라라고? 나보고 널 더 잘 만들어보라는거야? 어떻게 해야 네가 더 똑똑해지는데?\n",
      "Chatbot Response: 중요한 건 노력하는 과정이에요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'중요한 건 노력하는 과정이에요 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"노력에 따라라고? 나보고 널 더 잘 만들어보라는거야? 어떻게 해야 네가 더 똑똑해지는데?\", transformer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179dfda",
   "metadata": {},
   "source": [
    "## 7. 하이퍼파라미터 변경 실험_2\n",
    "\n",
    "\n",
    "여전히 이상함...   \n",
    "학습 진행을 살펴보았을 때 과적합이 일어난 것 같으니 dropout 상향해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a0d640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restored: ./checkpoints/train/ckpt-106\n",
      "Epoch 1 Batch 0 Loss 0.0195 Accuracy 0.9955\n",
      "Epoch 1 Batch 100 Loss 0.0192 Accuracy 0.9931\n",
      "Epoch 1 Loss 0.0102 Accuracy 0.9970\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-107\n",
      "Epoch 2 Batch 0 Loss 0.0034 Accuracy 0.9979\n",
      "Epoch 2 Batch 100 Loss 0.0035 Accuracy 1.0000\n",
      "Epoch 2 Loss 0.0106 Accuracy 0.9971\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-108\n",
      "Epoch 3 Batch 0 Loss 0.0056 Accuracy 0.9979\n",
      "Epoch 3 Batch 100 Loss 0.0040 Accuracy 0.9979\n",
      "Epoch 3 Loss 0.0109 Accuracy 0.9969\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-109\n",
      "Epoch 4 Batch 0 Loss 0.0034 Accuracy 1.0000\n",
      "Epoch 4 Batch 100 Loss 0.0206 Accuracy 0.9928\n",
      "Epoch 4 Loss 0.0094 Accuracy 0.9971\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-110\n",
      "Epoch 5 Batch 0 Loss 0.0053 Accuracy 1.0000\n",
      "Epoch 5 Batch 100 Loss 0.0094 Accuracy 0.9976\n",
      "Epoch 5 Loss 0.0102 Accuracy 0.9971\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-111\n",
      "Epoch 6 Batch 0 Loss 0.0045 Accuracy 0.9975\n",
      "Epoch 6 Batch 100 Loss 0.0128 Accuracy 0.9976\n",
      "Epoch 6 Loss 0.0097 Accuracy 0.9972\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-112\n",
      "Epoch 7 Batch 0 Loss 0.0106 Accuracy 0.9975\n",
      "Epoch 7 Batch 100 Loss 0.0259 Accuracy 0.9958\n",
      "Epoch 7 Loss 0.0093 Accuracy 0.9971\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/train/ckpt-113\n",
      "Epoch 8 Batch 0 Loss 0.0008 Accuracy 1.0000\n",
      "Epoch 8 Batch 100 Loss 0.0076 Accuracy 0.9977\n",
      "Epoch 8 Loss 0.0092 Accuracy 0.9972\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-114\n",
      "Epoch 9 Batch 0 Loss 0.0096 Accuracy 0.9929\n",
      "Epoch 9 Batch 100 Loss 0.0080 Accuracy 0.9975\n",
      "Epoch 9 Loss 0.0093 Accuracy 0.9971\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-115\n",
      "Epoch 10 Batch 0 Loss 0.0020 Accuracy 1.0000\n",
      "Epoch 10 Batch 100 Loss 0.0141 Accuracy 0.9955\n",
      "Epoch 10 Loss 0.0090 Accuracy 0.9973\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-116\n",
      "Epoch 11 Batch 0 Loss 0.0035 Accuracy 0.9977\n",
      "Epoch 11 Batch 100 Loss 0.0013 Accuracy 1.0000\n",
      "Epoch 11 Loss 0.0095 Accuracy 0.9970\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/train/ckpt-117\n",
      "Epoch 12 Batch 0 Loss 0.0010 Accuracy 1.0000\n",
      "Epoch 12 Batch 100 Loss 0.0174 Accuracy 0.9955\n",
      "Epoch 12 Loss 0.0086 Accuracy 0.9972\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/train/ckpt-118\n",
      "Epoch 13 Batch 0 Loss 0.0194 Accuracy 0.9932\n",
      "Epoch 13 Batch 100 Loss 0.0104 Accuracy 0.9978\n",
      "Epoch 13 Loss 0.0082 Accuracy 0.9974\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/train/ckpt-119\n",
      "Epoch 14 Batch 0 Loss 0.0126 Accuracy 0.9956\n",
      "Epoch 14 Batch 100 Loss 0.0009 Accuracy 1.0000\n",
      "Epoch 14 Loss 0.0092 Accuracy 0.9972\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/train/ckpt-120\n",
      "Epoch 15 Batch 0 Loss 0.0076 Accuracy 0.9953\n",
      "Epoch 15 Batch 100 Loss 0.0062 Accuracy 0.9977\n",
      "Epoch 15 Loss 0.0093 Accuracy 0.9972\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-121\n",
      "Epoch 16 Batch 0 Loss 0.0085 Accuracy 0.9957\n",
      "Epoch 16 Batch 100 Loss 0.0011 Accuracy 1.0000\n",
      "Epoch 16 Loss 0.0082 Accuracy 0.9973\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/train/ckpt-122\n",
      "Epoch 17 Batch 0 Loss 0.0034 Accuracy 0.9978\n",
      "Epoch 17 Batch 100 Loss 0.0115 Accuracy 0.9958\n",
      "Epoch 17 Loss 0.0086 Accuracy 0.9973\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/train/ckpt-123\n",
      "Epoch 18 Batch 0 Loss 0.0121 Accuracy 0.9956\n",
      "Epoch 18 Batch 100 Loss 0.0010 Accuracy 1.0000\n",
      "Epoch 18 Loss 0.0088 Accuracy 0.9973\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/train/ckpt-124\n",
      "Epoch 19 Batch 0 Loss 0.0034 Accuracy 0.9978\n",
      "Epoch 19 Batch 100 Loss 0.0090 Accuracy 0.9979\n",
      "Epoch 19 Loss 0.0080 Accuracy 0.9973\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/train/ckpt-125\n",
      "Epoch 20 Batch 0 Loss 0.0176 Accuracy 0.9929\n",
      "Epoch 20 Batch 100 Loss 0.0030 Accuracy 1.0000\n",
      "Epoch 20 Loss 0.0084 Accuracy 0.9975\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-126\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 변경\n",
    "DROPOUT = 0.3 ## 0.1 -> 0.3 상향 (과적합을 억제하고 일반화 성능 향상)\n",
    "\n",
    "# 트랜스포머 모델 생성\n",
    "transformer_3 = Transformer(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dff=DFF,\n",
    "    input_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    target_vocab_size=TARGET_VOCAB_SIZE,\n",
    "    pe_input=MAX_POS_ENCODING,\n",
    "    pe_target=MAX_POS_ENCODING\n",
    ")\n",
    "\n",
    "\n",
    "# 체크포인트 설정\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt_3 = tf.train.Checkpoint(transformer=transformer_3, optimizer=optimizer)\n",
    "ckpt_manager_3 = tf.train.CheckpointManager(ckpt_3, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 이전 체크포인트가 있으면 복원\n",
    "if ckpt_manager_3.latest_checkpoint:\n",
    "    ckpt_3.restore(ckpt_manager_3.latest_checkpoint)\n",
    "    print(\"Checkpoint restored:\", ckpt_manager_3.latest_checkpoint)\n",
    "    \n",
    "\n",
    "# 학습 실행\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for batch, (features, labels) in enumerate(dataset):\n",
    "        batch_loss, batch_accuracy = train_step(features, labels)\n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Batch {batch} Loss {batch_loss:.4f} Accuracy {batch_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss {total_loss / (batch + 1):.4f} Accuracy {total_accuracy / (batch + 1):.4f}\")\n",
    "\n",
    "    # 체크포인트 저장\n",
    "    ckpt_save_path = ckpt_manager_3.save()\n",
    "    print(f\"Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "137004db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 너는 누구야?\n",
      "Chatbot Response: 저는 마음을 이어주는 위로봇입니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저는 마음을 이어주는 위로봇입니다 .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"너는 누구야?\", transformer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0409ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 배고파\n",
      "Chatbot Response: 뭐 좀 챙겨드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'뭐 좀 챙겨드세요 .'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"배고파\", transformer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a941105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 지금은 11시 30분이고 점심시간은 12시 50분부터인데 지금부터 점심을 미리 먹어도 될까?\n",
      "Chatbot Response: 정신 차리세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정신 차리세요 .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"지금은 11시 30분이고 점심시간은 12시 50분부터인데 지금부터 점심을 미리 먹어도 될까?\", transformer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ff8cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input: 밥 빨리 먹고싶어ㅠㅠ\n",
      "Chatbot Response: 엄청난 용기가 필요하겠네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'엄청난 용기가 필요하겠네요 .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"밥 빨리 먹고싶어ㅠㅠ\", transformer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5755d8c",
   "metadata": {},
   "source": [
    "## 8. 회고\n",
    "\n",
    "개인적으로 자연어처리는 너무 어려운 것 같음.\n",
    "오류도 많이나고 이상한 결과가 타나나는 것을 확인함.\n",
    "해원님 코드로 겨우 완성본 확인함.\n",
    "자연어 처리는 기초부터 다시 공부할 필요가 있을 것 같음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
